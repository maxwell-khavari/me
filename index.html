<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Maxwell Khavari – AI & Fairness Projects</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&family=Playfair+Display:wght@700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-dark: #0e1a2b;
            --secondary-gray: #1e1e1e;
            --accent-blue: #2e5dff;
            --text-light: #eaeaea;
            --text-muted: #bbbbbb;
            --card-bg: #121212;
            --border-glow: #2e5dff;
            --shadow: 0 8px 24px rgba(0, 0, 0, 0.2);
        }

        * {
            box-sizing: border-box;
        }

        body {
            margin: 0;
            font-family: 'Roboto', sans-serif;
            background-color: var(--secondary-gray);
            color: var(--text-light);
            line-height: 1.6;
        }

        header {
            background: var(--primary-dark);
            padding: 3rem 1rem;
            text-align: center;
            box-shadow: var(--shadow);
            border-bottom: 2px solid var(--accent-blue);
        }

        header h1 {
            font-family: 'Playfair Display', serif;
            font-size: 3.5rem;
            margin: 0.2rem 0;
            color: var(--accent-blue);
        }

        header p {
            font-size: 1.2rem;
            font-weight: 300;
            color: var(--text-muted);
        }

        header img {
            max-width: 130px;
            border-radius: 50%;
            border: 4px solid var(--accent-blue);
            margin-top: 1rem;
            box-shadow: 0 2px 20px rgba(46, 93, 255, 0.3);
        }

        .container {
            max-width: 960px;
            padding: 0 1.5rem;
            margin: auto;
        }

        section {
            background: var(--card-bg);
            padding: 2rem;
            border-radius: 10px;
            margin-bottom: 2rem;
            border-left: 4px solid var(--accent-blue);
            box-shadow: var(--shadow);
            transition: transform 0.2s ease;
        }

        section:hover {
            transform: scale(1.005);
        }

        h2 {
            font-family: 'Playfair Display', serif;
            font-size: 2.2rem;
            color: var(--accent-blue);
            margin-top: 0;
            border-bottom: 1px solid #333;
            padding-bottom: 0.5rem;
        }

        h3 {
            font-size: 1.5rem;
            color: var(--text-light);
            margin-top: 1.5rem;
        }

        ul {
            padding-left: 20px;
        }

        ul li {
            margin-bottom: 0.6rem;
        }

        a {
            color: var(--accent-blue);
            text-decoration: none;
        }

        .projects a, .cta-button {
            display: block;
            margin: 1rem 0;
            padding: 0.8rem 1.2rem;
            background: var(--accent-blue);
            color: white;
            font-weight: bold;
            border-radius: 5px;
            text-align: center;
            transition: background 0.3s ease, transform 0.2s ease;
        }

        .projects a.github-link {
            background: #333;
        }

        .projects a:hover, .cta-button:hover {
            background: #1d3edd;
            transform: translateY(-2px);
        }

        .status-update {
            background-color: #1a1a1a;
            border-left: 5px solid #ffc107;
            padding: 1rem 1.5rem;
            margin-bottom: 1.5rem;
            border-radius: 6px;
            color: #ffdb6e;
            font-style: italic;
        }

        footer {
            text-align: center;
            padding: 2rem 1rem;
            font-size: 0.9rem;
            color: var(--text-muted);
            background-color: var(--primary-dark);
        }

        footer a {
            color: var(--accent-blue);
            font-weight: bold;
        }

        footer a:hover {
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            header h1 {
                font-size: 2.4rem;
            }
            h2 {
                font-size: 1.8rem;
            }
            h3 {
                font-size: 1.3rem;
            }
        }
    </style>
</head>
<body>

    <header>
        <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/49/Stanford_University_seal_2003.svg/1200px-Stanford_University_seal_2003.svg.png" alt="Stanford University Logo">
        <h1>Maxwell Khavari</h1>
        <p>Researcher • Developer • AI Fairness Advocate</p>
    </header>

    <div class="container">

        <section>
            <h2>About My Work</h2>
            <p>
                I focus on the intersection of AI fairness, computer vision, and neural network design — especially in how bias manifests across text-to-image generation and image classification pipelines. These systems, while incredibly powerful, often amplify societal biases if not actively interrogated and redesigned.
            </p>
            <p>
                My research explores how race, gender, and socioeconomic status are represented in both generated imagery and classifier outputs. I develop tooling to reverse-engineer prompts, analyze representation trends, and quantify human perceptions of bias.
            </p>
            <p>
                In addition to auditing third-party models, I also build and experiment with my own neural networks — particularly convolutional neural networks (CNNs) for image analysis. I’ve trained small-scale models in Google Colab using datasets like CIFAR-100 to study classification fairness and feature encoding across demographic traits.
            </p>
        </section>

        <section>
            <h2>Current Project: Algorithmic Bias Progression Over Time</h2>

            <h3>The Problem & Opportunity</h3>
            <ul>
                <li><strong>Persistent Algorithmic Bias:</strong> Text-to-image AIs generate biased visuals (e.g., for professions, leadership) even from neutral prompts like "CEO" or "nurse."</li>
                <li><strong>Temporal Blind Spot:</strong> There's limited understanding of how these biases evolve across model versions.</li>
                <li><strong>Bias Injection at the Source:</strong> Language models can introduce bias even before the image generation begins.</li>
                <li><strong>Human Perception Gap:</strong> What appears neutral to one person may feel offensive to another — especially across racial and gender lines.</li>
            </ul>

            <h3>Core Hypothesis</h3>
            <p>
                Models exhibit persistent and shifting bias tied to leadership, prestige, race, and gender — even when prompts are neutral. These effects are experienced differently across evaluators with distinct identities.
            </p>

            <h3>Project Goals</h3>
            <ul>
                <li>Quantify representation shifts across model versions over time.</li>
                <li>Document patterns (increasing, decreasing, stable bias).</li>
                <li>Measure differences in perceived bias by race/gender of evaluators.</li>
            </ul>

            <div class="status-update">
                Current Status: Automation and analysis scripts are nearly complete. Final steps involve swapping locators for different image models before full dataset generation.
            </div>

            <a href="INSERT_LINK_HERE" class="cta-button">View Full Project PRD</a>
        </section>

        <section>
            <h2>Project Components & Methodology Highlights</h2>

            <h3>Core Domains for Bias Analysis ("Rule of 3")</h3>
            <ul>
                <li><strong>Medical Professions:</strong> Nurse, surgeon, paramedic – examining prestige and gender/race encoding.</li>
                <li><strong>Students & Patients:</strong> Often infantilized or racialized — testing model assumptions of passivity or vulnerability.</li>
                <li><strong>Leaders & Experts:</strong> Who gets visualized as a "CEO," "icon," or "intellectual"?</li>
            </ul>

            <h3>Key Analysis Methods</h3>
            <ul>
                <li><strong>Automated Generation:</strong> Playwright + Node.js for rendering 1000s of prompts across platforms.</li>
                <li><strong>Bias Tagging:</strong> FairFace & DeepFace used to classify race, gender, and age automatically.</li>
                <li><strong>Prompt Reconstruction:</strong> Interrogate CLIP, BLIP-2, InstructBLIP to understand model interpretation.</li>
                <li><strong>Human Perception:</strong> A cross-demographic panel rates each image for fairness and representation.</li>
            </ul>

            <h3>My Modeling Tools</h3>
            <ul>
                <li><strong>CNNs:</strong> Built in PyTorch/Keras for race/gender detection and fairness experiments.</li>
                <li><strong>Colab Experiments:</strong> CIFAR-100 classifiers trained and tested in Colab to simulate small-scale models.</li>
                <li><strong>Classifiers:</strong> Evaluating what CNNs learn from seemingly neutral features like lighting, clothing, or background.</li>
            </ul>
        </section>

        <section>
            <h2>My GitHub Repositories</h2>
            <div class="projects">
                <a href="https://github.com/maxwell-khavari/ai-dataset-automation-script" class="github-link">Algorithmic Bias Progression (Main Project)</a>
                <a href="https://github.com/maxwell-khavari/classify" class="github-link">Race & Gender Tagger</a>
                <a href="https://github.com/maxwell-khavari/cifar100_ai_image_models" class="github-link">CNN Model (Colab)</a>
            </div>
        </section>

    </div>

    <footer>
        &copy; 2025 Maxwell Khavari • <a href="https://github.com/maxwell-khavari">GitHub Profile</a><br>
        <small>Summer Internship @ BMDS</small>
    </footer>

</body>
</html>
